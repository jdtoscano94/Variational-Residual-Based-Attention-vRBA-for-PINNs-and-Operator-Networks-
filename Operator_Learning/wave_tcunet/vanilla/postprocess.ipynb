{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5553ccd8-fb9b-4eee-8125-cec46f3ab5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3026928/838106139.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tcunet import Unet2D\n",
    "# from YourDataset import YourDataset  # Import your custom dataset here\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchinfo import summary\n",
    "\n",
    "torch.manual_seed(23)\n",
    "\n",
    "import pickle\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d07322-3d17-4a4f-9f07-82d44729814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom loss function here\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true, Par, Lambda=None):\n",
    "        # Implement your custom loss calculation here\n",
    "        if Lambda is not None:\n",
    "            residue = torch.absolute(y_true - y_pred)\n",
    "            Lambda = Par['gamma']*Lambda + Par['eta']*residue/torch.max(residue)\n",
    "            # loss = torch.mean(torch.square(Lambda*residue)) \n",
    "            loss = torch.norm(Lambda*residue, p=2)/torch.norm(y_true, p=2)\n",
    "        \n",
    "        else:\n",
    "            # loss = torch.mean(torch.square(y_true - y_pred)) \n",
    "            loss = torch.norm(y_true-y_pred, p=2)/torch.norm(y_true, p=2)\n",
    "\n",
    "        return loss, Lambda\n",
    "\n",
    "class YourDataset(Dataset):\n",
    "    def __init__(self, x, t, y, transform=None):\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_sample = self.x[idx]\n",
    "        t_sample = self.t[idx]\n",
    "        y_sample = self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x_sample, t_sample, y_sample = self.transform(x_sample, t_sample, y_sample)\n",
    "\n",
    "        return x_sample, t_sample, y_sample\n",
    "\n",
    "class YourDataset_L(Dataset):\n",
    "    def __init__(self, x, t, y, Lambda, transform=None):\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        self.y = y\n",
    "        self.Lambda = Lambda\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_sample = self.x[idx]\n",
    "        t_sample = self.t[idx]\n",
    "        y_sample = self.y[idx]\n",
    "        Lambda_sample = self.Lambda[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x_sample, t_sample, y_sample = self.transform(x_sample, t_sample, y_sample)\n",
    "\n",
    "        return x_sample, t_sample, y_sample, Lambda_sample, idx\n",
    "\n",
    "def preprocess(traj, Par):\n",
    "    x = sliding_window_view(traj[:,:-(Par['lf']-1),:,:], window_shape=Par['lb'], axis=1 ).transpose(0,1,4,2,3).reshape(-1,Par['lb'],Par['nx'], Par['ny'])\n",
    "    y = sliding_window_view(traj[:,Par['lb']-1:,:,:], window_shape=Par['lf'], axis=1 ).transpose(0,1,4,2,3).reshape(-1,Par['lf'],Par['nx'], Par['ny'])\n",
    "    t = np.linspace(0,1,Par['lf']).reshape(-1,1)\n",
    "\n",
    "    nt = y.shape[1]\n",
    "    n_samples = y.shape[0]\n",
    "\n",
    "    t = np.tile(t, [n_samples,1]).reshape(-1,)                                                     #[_*nt, ]\n",
    "    x = np.repeat(x,nt, axis=0)                                   #[_*nt, 1, 64, 64]\n",
    "    y = y.reshape(y.shape[0]*y.shape[1],1,y.shape[2],y.shape[3])  #[_*nt, 64, 64]\n",
    "\n",
    "\n",
    "    print('x: ', x.shape)\n",
    "    print('y: ', y.shape)\n",
    "    print('t: ', t.shape)\n",
    "    print()\n",
    "    return x,y,t\n",
    "\n",
    "def get_flat_gradients(param_tensors):\n",
    "    grad_list = []\n",
    "    for p in param_tensors:\n",
    "        if p.grad is not None:\n",
    "            grad_list.append(p.grad.view(-1))\n",
    "    flat_gradients = torch.cat(grad_list)\n",
    "    return flat_gradients\n",
    "\n",
    "def get_snr(L_theta_ls):\n",
    "    L_theta = np.concatenate(L_theta_ls, axis=0) #[NB, W]\n",
    "    L_theta = np.nan_to_num(L_theta, nan=0.0, posinf=1e12, neginf=-1e12)\n",
    "    mu  = np.mean(L_theta, axis=0) #[W,]\n",
    "    sig = np.std(L_theta, axis=0)  #[W,]\n",
    "    NUM = np.linalg.norm(mu, ord=2)\n",
    "    DEN = np.linalg.norm(sig, ord=2)\n",
    "    snr = NUM/DEN\n",
    "\n",
    "    # Save MU and STD as well!\n",
    "    # Do not use GradScaler for calculating SNR\n",
    "    # Set gamma as gamma = 1 - eta\n",
    "\n",
    "    if np.isnan(L_theta).any():\n",
    "        print(f\"Warning: NaN detected in gradients at L_theta\")\n",
    "    if np.isinf(L_theta).any():\n",
    "        print(f\"Warning: inf detected in gradients at L_theta\")\n",
    "\n",
    "    return snr, NUM, DEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824ccae2-4463-43c1-8d12-e36422d35b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Dataset\n",
      "x:  (40800, 1, 64, 64)\n",
      "y:  (40800, 1, 64, 64)\n",
      "t:  (40800,)\n",
      "\n",
      "\n",
      "Validation Dataset\n",
      "x:  (5100, 1, 64, 64)\n",
      "y:  (5100, 1, 64, 64)\n",
      "t:  (5100,)\n",
      "\n",
      "\n",
      "Test Dataset\n",
      "x:  (5100, 1, 64, 64)\n",
      "y:  (5100, 1, 64, 64)\n",
      "t:  (5100,)\n",
      "\n",
      "Lambda:  (40800, 1, 64, 64)\n",
      "Par: \n",
      " {'nx': 64, 'ny': 64, 'nf': 1, 'd_emb': 128, 'lb': 1, 'lf': 51, 'num_epochs': 500, 'inp_shift': np.float64(0.007669870189599345), 'inp_scale': np.float64(0.061450183570653995), 'out_shift': np.float64(-0.001635048307912137), 'out_scale': np.float64(0.03807830166415873), 't_shift': np.float64(0.0), 't_scale': np.float64(1.0), 'gamma': 0.99, 'eta': 0.010000000000000009, 'do_rba': False, 'get_snr': True, 'Lambda_max': 1.0}\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "# Load your data into NumPy arrays (x_train, t_train, y_train, x_val, t_val, y_val, x_test, t_test, y_test)\n",
    "#########################\n",
    "x = np.load('../data/x.npy')  #[_, 64, 64]\n",
    "t = np.load('../data/t.npy')  #[_, 200]\n",
    "y = np.load('../data/y.npy')  #[_, 64, 64, 200]\n",
    "\n",
    "if debug:\n",
    "    x = x[:100]\n",
    "    y = y[:100]\n",
    "\n",
    "idx1 = int(0.8 * x.shape[0])\n",
    "idx2 = int(0.9 * x.shape[0])\n",
    "\n",
    "traj = np.append( np.expand_dims(x, axis=-1), y, axis=-1 ).transpose(0,3,1,2) #[_, 64, 64, 201]\n",
    "\n",
    "traj_train = traj[:idx1, ::4]\n",
    "traj_val   = traj[idx1:idx2, ::4]\n",
    "traj_test  = traj[idx2:, ::4]\n",
    "\n",
    "Par = {}\n",
    "# Par['nt'] = 100 \n",
    "Par['nx'] = traj_train.shape[2]\n",
    "Par['ny'] = traj_train.shape[3]\n",
    "Par['nf'] = 1\n",
    "Par['d_emb'] = 128\n",
    "\n",
    "Par['lb'] = 1\n",
    "Par['lf'] = 51 \n",
    "# Par['temp'] = Par['nt'] - Par['lb'] - Par['lf'] + 2\n",
    "Par['num_epochs'] = 500\n",
    "if debug:\n",
    "    Par['num_epochs'] = 5 \n",
    "\n",
    "print('\\nTrain Dataset')\n",
    "x_train, y_train, t_train = preprocess(traj_train, Par)\n",
    "print('\\nValidation Dataset')\n",
    "x_val, y_val, t_val  = preprocess(traj_val, Par)\n",
    "print('\\nTest Dataset')\n",
    "x_test, y_test, t_test  = preprocess(traj_test, Par)\n",
    "\n",
    "t_min = np.min(t_train)\n",
    "t_max = np.max(t_train)\n",
    "\n",
    "Par['inp_shift'] = np.mean(x_train) \n",
    "Par['inp_scale'] = np.std(x_train)\n",
    "Par['out_shift'] = np.mean(y_train)\n",
    "Par['out_scale'] = np.std(y_train)\n",
    "Par['t_shift']   = t_min\n",
    "Par['t_scale']   = t_max - t_min\n",
    "\n",
    "# Par['eta']   = 0.1 #use 0.01 so that eta = 1-gamma\n",
    "Par['gamma'] = 0.99\n",
    "Par['eta']   = 1 - Par['gamma']\n",
    "\n",
    "Par['do_rba']  = False\n",
    "Par['get_snr'] = True\n",
    "\n",
    "Par['Lambda_max'] = Par['eta']/(1 - Par['gamma'])\n",
    "\n",
    "Lambda = np.ones(y_train.shape, dtype=np.float32)*Par['Lambda_max']/2.0\n",
    "print(\"Lambda: \", Lambda.shape)\n",
    "\n",
    "print(\"Par: \\n\", Par)\n",
    "\n",
    "with open('Par.pkl', 'wb') as f:\n",
    "    pickle.dump(Par, f)\n",
    "\n",
    "# sys.exit()\n",
    "#########################\n",
    "\n",
    "# Create custom datasets\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "t_train_tensor = torch.tensor(t_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "Lambda_tensor  = torch.tensor(Lambda, dtype=torch.float32)\n",
    "\n",
    "x_val_tensor   = torch.tensor(x_val,   dtype=torch.float32)\n",
    "t_val_tensor   = torch.tensor(t_val,   dtype=torch.float32)\n",
    "y_val_tensor   = torch.tensor(y_val,   dtype=torch.float32)\n",
    "\n",
    "x_test_tensor  = torch.tensor(x_test,  dtype=torch.float32)\n",
    "t_test_tensor  = torch.tensor(t_test,  dtype=torch.float32)\n",
    "y_test_tensor  = torch.tensor(y_test,  dtype=torch.float32)\n",
    "\n",
    "train_dataset = YourDataset_L(x_train_tensor, t_train_tensor, y_train_tensor, Lambda_tensor)\n",
    "val_dataset = YourDataset(x_val_tensor, t_val_tensor, y_val_tensor)\n",
    "test_dataset = YourDataset(x_test_tensor, t_test_tensor, y_test_tensor)\n",
    "\n",
    "# Define data loaders\n",
    "train_batch_size = 100\n",
    "val_batch_size   = 100\n",
    "test_batch_size  = 100\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=val_batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35802e41-9b94-44dc-8ac0-903a080bd94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "Unet2D                                                  [1, 1, 64, 64]            --\n",
      "├─Conv2d: 1-1                                           [1, 16, 64, 64]           2,368\n",
      "├─Sequential: 1-2                                       [1, 64]                   --\n",
      "│    └─SinusoidalPosEmb: 2-1                            [1, 16]                   --\n",
      "│    └─Linear: 2-2                                      [1, 64]                   1,088\n",
      "│    └─GELU: 2-3                                        [1, 64]                   --\n",
      "│    └─Linear: 2-4                                      [1, 64]                   4,160\n",
      "├─ModuleList: 1-3                                       --                        --\n",
      "│    └─ModuleList: 2-5                                  --                        --\n",
      "│    │    └─ResnetBlock: 3-1                            [1, 16, 64, 64]           6,784\n",
      "│    │    └─ResnetBlock: 3-2                            [1, 16, 64, 64]           6,784\n",
      "│    │    └─Residual: 3-3                               [1, 16, 64, 64]           8,240\n",
      "│    │    └─Sequential: 3-4                             [1, 16, 32, 32]           1,040\n",
      "│    └─ModuleList: 2-6                                  --                        --\n",
      "│    │    └─ResnetBlock: 3-5                            [1, 16, 32, 32]           6,784\n",
      "│    │    └─ResnetBlock: 3-6                            [1, 16, 32, 32]           6,784\n",
      "│    │    └─Residual: 3-7                               [1, 16, 32, 32]           8,240\n",
      "│    │    └─Sequential: 3-8                             [1, 32, 16, 16]           2,080\n",
      "│    └─ModuleList: 2-7                                  --                        --\n",
      "│    │    └─ResnetBlock: 3-9                            [1, 32, 16, 16]           22,784\n",
      "│    │    └─ResnetBlock: 3-10                           [1, 32, 16, 16]           22,784\n",
      "│    │    └─Residual: 3-11                              [1, 32, 16, 16]           16,480\n",
      "│    │    └─Sequential: 3-12                            [1, 64, 8, 8]             8,256\n",
      "│    └─ModuleList: 2-8                                  --                        --\n",
      "│    │    └─ResnetBlock: 3-13                           [1, 64, 8, 8]             82,432\n",
      "│    │    └─ResnetBlock: 3-14                           [1, 64, 8, 8]             82,432\n",
      "│    │    └─Residual: 3-15                              [1, 64, 8, 8]             32,960\n",
      "│    │    └─Conv2d: 3-16                                [1, 128, 8, 8]            73,856\n",
      "├─ResnetBlock: 1-4                                      [1, 128, 8, 8]            --\n",
      "│    └─Sequential: 2-9                                  [1, 256]                  --\n",
      "│    │    └─SiLU: 3-17                                  [1, 64]                   --\n",
      "│    │    └─Linear: 3-18                                [1, 256]                  16,640\n",
      "│    └─Block: 2-10                                      [1, 128, 8, 8]            --\n",
      "│    │    └─Conv2d: 3-19                                [1, 128, 8, 8]            147,584\n",
      "│    │    └─GroupNorm: 3-20                             [1, 128, 8, 8]            256\n",
      "│    │    └─SiLU: 3-21                                  [1, 128, 8, 8]            --\n",
      "│    └─Block: 2-11                                      [1, 128, 8, 8]            --\n",
      "│    │    └─Conv2d: 3-22                                [1, 128, 8, 8]            147,584\n",
      "│    │    └─GroupNorm: 3-23                             [1, 128, 8, 8]            256\n",
      "│    │    └─SiLU: 3-24                                  [1, 128, 8, 8]            --\n",
      "│    └─Identity: 2-12                                   [1, 128, 8, 8]            --\n",
      "├─Residual: 1-5                                         [1, 128, 8, 8]            --\n",
      "│    └─PreNorm: 2-13                                    [1, 128, 8, 8]            --\n",
      "│    │    └─LayerNorm: 3-25                             [1, 128, 8, 8]            128\n",
      "│    │    └─Attention: 3-26                             [1, 128, 8, 8]            65,664\n",
      "├─ResnetBlock: 1-6                                      [1, 128, 8, 8]            --\n",
      "│    └─Sequential: 2-14                                 [1, 256]                  --\n",
      "│    │    └─SiLU: 3-27                                  [1, 64]                   --\n",
      "│    │    └─Linear: 3-28                                [1, 256]                  16,640\n",
      "│    └─Block: 2-15                                      [1, 128, 8, 8]            --\n",
      "│    │    └─Conv2d: 3-29                                [1, 128, 8, 8]            147,584\n",
      "│    │    └─GroupNorm: 3-30                             [1, 128, 8, 8]            256\n",
      "│    │    └─SiLU: 3-31                                  [1, 128, 8, 8]            --\n",
      "│    └─Block: 2-16                                      [1, 128, 8, 8]            --\n",
      "│    │    └─Conv2d: 3-32                                [1, 128, 8, 8]            147,584\n",
      "│    │    └─GroupNorm: 3-33                             [1, 128, 8, 8]            256\n",
      "│    │    └─SiLU: 3-34                                  [1, 128, 8, 8]            --\n",
      "│    └─Identity: 2-17                                   [1, 128, 8, 8]            --\n",
      "├─ModuleList: 1-7                                       --                        --\n",
      "│    └─ModuleList: 2-18                                 --                        --\n",
      "│    │    └─ResnetBlock: 3-35                           [1, 128, 8, 8]            410,752\n",
      "│    │    └─ResnetBlock: 3-36                           [1, 128, 8, 8]            410,752\n",
      "│    │    └─Residual: 3-37                              [1, 128, 8, 8]            65,920\n",
      "│    │    └─Sequential: 3-38                            [1, 64, 16, 16]           73,792\n",
      "│    └─ModuleList: 2-19                                 --                        --\n",
      "│    │    └─ResnetBlock: 3-39                           [1, 64, 16, 16]           107,072\n",
      "│    │    └─ResnetBlock: 3-40                           [1, 64, 16, 16]           107,072\n",
      "│    │    └─Residual: 3-41                              [1, 64, 16, 16]           32,960\n",
      "│    │    └─Sequential: 3-42                            [1, 32, 32, 32]           18,464\n",
      "│    └─ModuleList: 2-20                                 --                        --\n",
      "│    │    └─ResnetBlock: 3-43                           [1, 32, 32, 32]           28,960\n",
      "│    │    └─ResnetBlock: 3-44                           [1, 32, 32, 32]           28,960\n",
      "│    │    └─Residual: 3-45                              [1, 32, 32, 32]           16,480\n",
      "│    │    └─Sequential: 3-46                            [1, 16, 64, 64]           4,624\n",
      "│    └─ModuleList: 2-21                                 --                        --\n",
      "│    │    └─ResnetBlock: 3-47                           [1, 16, 64, 64]           9,616\n",
      "│    │    └─ResnetBlock: 3-48                           [1, 16, 64, 64]           9,616\n",
      "│    │    └─Residual: 3-49                              [1, 16, 64, 64]           8,240\n",
      "│    │    └─Conv2d: 3-50                                [1, 16, 64, 64]           2,320\n",
      "├─ResnetBlock: 1-8                                      [1, 16, 64, 64]           --\n",
      "│    └─Sequential: 2-22                                 [1, 32]                   --\n",
      "│    │    └─SiLU: 3-51                                  [1, 64]                   --\n",
      "│    │    └─Linear: 3-52                                [1, 32]                   2,080\n",
      "│    └─Block: 2-23                                      [1, 16, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-53                                [1, 16, 64, 64]           4,624\n",
      "│    │    └─GroupNorm: 3-54                             [1, 16, 64, 64]           32\n",
      "│    │    └─SiLU: 3-55                                  [1, 16, 64, 64]           --\n",
      "│    └─Block: 2-24                                      [1, 16, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-56                                [1, 16, 64, 64]           2,320\n",
      "│    │    └─GroupNorm: 3-57                             [1, 16, 64, 64]           32\n",
      "│    │    └─SiLU: 3-58                                  [1, 16, 64, 64]           --\n",
      "│    └─Conv2d: 2-25                                     [1, 16, 64, 64]           528\n",
      "├─Conv2d: 1-9                                           [1, 1, 64, 64]            17\n",
      "=========================================================================================================\n",
      "Total params: 2,432,001\n",
      "Trainable params: 2,432,001\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 545.94\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 60.28\n",
      "Params size (MB): 9.73\n",
      "Estimated Total Size (MB): 70.02\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize your Unet2D model\n",
    "model = Unet2D(dim=16, Par=Par, dim_mults=(1, 2, 4, 8)).to(device).to(torch.float32)\n",
    "print(summary(model, input_size=((1,)+x_train.shape[1:], (1,))  ))\n",
    "\n",
    "path_model = 'models/best_model.pt'\n",
    "model.load_state_dict(torch.load(path_model))\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = CustomLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4051cb5c-989c-48bf-8c9e-f69f28ba6746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3026928/1345831285.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.6775e-02\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for x, t, y_true in val_loader:\n",
    "        with autocast():\n",
    "            y_pred = model(x.to(device), t.to(device))\n",
    "            loss, _   = criterion(y_pred, y_true.to(device), Par)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "val_loss /= len(val_loader)\n",
    "print(f'Val Loss: {val_loss:.4e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e96513a-7007-4987-a708-d0793bbb106d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3026928/3873827287.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.6934e-02\n"
     ]
    }
   ],
   "source": [
    "y_true_ls = []\n",
    "y_pred_ls = []\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for x, t, y_true in test_loader:\n",
    "        with autocast():\n",
    "            y_pred = model(x.to(device), t.to(device))\n",
    "            loss, _ = criterion(y_pred, y_true.to(device), Par)\n",
    "        \n",
    "            y_true_ls.append(y_true)\n",
    "            y_pred_ls.append(y_pred)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74852dd2-ab51-4390-a898-73ed49daa595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true:  (100, 51, 64, 64)\n",
      "pred:  (100, 51, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "Y_TRUE = torch.cat(y_true_ls, axis=0).reshape(-1,51,64,64).detach().cpu().numpy()\n",
    "Y_PRED = torch.cat(y_pred_ls, axis=0).reshape(-1,51,64,64).detach().cpu().numpy()\n",
    "\n",
    "print('true: ', Y_TRUE.shape)\n",
    "print('pred: ', Y_PRED.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7908bb9-80c7-427b-aabd-13a2800cef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Y_TRUE.npy\", Y_TRUE)\n",
    "np.save(\"Y_PRED.npy\", Y_PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8455bc46-34b0-4699-a6bf-526abd5293ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
